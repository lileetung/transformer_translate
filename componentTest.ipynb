{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大型语言模型（LLM，如BERT、GPT等）处理文本数据的一般流程。这个过程可以细分为以下几个步骤：\n",
    "\n",
    "1. **分词（Tokenization）**：\n",
    "   - 这一步骤是将文本字符串分解成较小的单位，称为“词元”（tokens）。分词可以根据不同的规则进行，如基于空格和标点符号分割、子词分割（如Byte-Pair Encoding, BPE）或使用更复杂的基于规则或学习的方法。\n",
    "\n",
    "2. **词元到ID的映射（Token to ID mapping）**：\n",
    "   - 分词后，每个词元会被转换成一个唯一的数字ID。这些ID对应于语言模型的词汇表中的索引。这一步是必要的，因为计算机模型不能直接处理文本数据，而是处理数字。\n",
    "\n",
    "3. **嵌入层（Embedding）**：\n",
    "   - 在将词元转换为ID之后，这些ID将被用来在嵌入层中查找每个词元的向量表示。嵌入层是一个可训练的参数矩阵，模型通过学习来调整这些向量，以便更好地捕捉词义和语法关系。\n",
    "   - 这些嵌入通常包括词嵌入，可能还包括位置嵌入（指示词元在句子中的位置）和分段嵌入（特别是在处理多个句子时）。\n",
    "\n",
    "4. **模型处理（Model Processing）**：\n",
    "   - 嵌入向量随后会被送入语言模型的主体，如Transformer结构。这些模型会通过一系列的层处理这些向量，每一层都包括例如自注意力机制和前馈神经网络等组件。模型通过这种方式来理解和生成文本上下文的表示。\n",
    "\n",
    "5. **输出解析（Output Decoding）**：\n",
    "   - 对于生成任务，如文本续写，模型的输出通常是在词汇表上的概率分布，这些概率表明下一个词元是什么。然后可以使用不同的策略（如贪婪搜索、束搜索）从这些概率中选择词元来生成文本。\n",
    "   - 对于分类任务（如情感分析），模型输出通常是分类标签的概率。\n",
    "\n",
    "6. **后处理（Post-processing）**：\n",
    "   - 在文本生成任务中，输出的词元序列通常会经过后处理转换回可读的文本。这可能包括去除特殊词元、调整格式等步骤。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.73M/5.73M [00:01<00:00, 3.62MB/s]\n",
      "Generating train split: 100%|██████████| 32332/32332 [00:00<00:00, 610329.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus_books\", 'en-it')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'translation': {'en': 'Source: Project Gutenberg', 'it': 'Source: www.liberliber.it/Audiobook available here'}}\n",
      "{'id': '1', 'translation': {'en': 'Jane Eyre', 'it': 'Jane Eyre'}}\n",
      "{'id': '2', 'translation': {'en': 'Charlotte Bronte', 'it': 'Charlotte Brontë'}}\n",
      "{'id': '3', 'translation': {'en': 'CHAPTER I', 'it': 'PARTE PRIMA'}}\n",
      "{'id': '4', 'translation': {'en': 'There was no possibility of taking a walk that day.', 'it': 'I. In quel giorno era impossibile passeggiare.'}}\n",
      "{'id': '5', 'translation': {'en': 'We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further out-door exercise was now out of the question.', 'it': \"La mattina avevamo errato per un'ora nel boschetto spogliato di foglie, ma dopo pranzo (quando non vi erano invitati, la signora Reed desinava presto), il vento gelato d'inverno aveva portato seco nubi così scure e una pioggia così penetrante, che non si poteva pensare a nessuna escursione.\"}}\n",
      "{'id': '6', 'translation': {'en': 'I was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the coming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the chidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to Eliza, John, and Georgiana Reed.', 'it': 'Ne ero contenta. Non mi sono mai piaciute le lunghe passeggiate, sopra tutto col freddo, ed era cosa penosa per me di tornar di notte con le mani e i piedi gelati, col cuore amareggiato dalle sgridate di Bessie, la bambinaia, e con lo spirito abbattuto dalla coscienza della mia inferiorità fisica di fronte a Eliza, a John e a Georgiana Reed.'}}\n",
      "{'id': '7', 'translation': {'en': 'The said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room: she lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither quarrelling nor crying) looked perfectly happy.', 'it': 'Eliza, John e Georgiana erano aggruppati in salotto attorno alla loro mamma; questa, sdraiata sul sofà accanto al fuoco e circondata dai suoi bambini, che in quel momento non questionavano fra loro né piangevano, pareva perfettamente felice.'}}\n",
      "{'id': '8', 'translation': {'en': 'Me, she had dispensed from joining the group; saying, \"She regretted to be under the necessity of keeping me at a distance; but that until she heard from Bessie, and could discover by her own observation, that I was endeavouring in good earnest to acquire a more sociable and childlike disposition, a more attractive and sprightly manner--something lighter, franker, more natural, as it were--she really must exclude me from privileges intended only for contented, happy, little children.\"', 'it': 'Ella mi aveva proibito di unirmi al loro gruppo, dicendo che deplorava la necessità in cui trovavasi di tenermi così lontana, ma che fino al momento in cui Bessie non guarentirebbe che mi studiavo di acquistare un carattere più socievole e più infantile, maniere più cortesi e qualcosa di più radioso, di più aperto, di più sincero, non poteva concedermi gli stessi privilegi che ai bambini allegri e soddisfatti.'}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'datasource': 'opus_books',\n",
    "    'lang_src': 'en',\n",
    "    'lang_tgt': 'it',\n",
    "    'tokenizer_file': 'tokenizer_{0}.json'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset, language):\n",
    "    for item in dataset:\n",
    "        yield item['translation'][language]\n",
    "\n",
    "def get_or_build_tokenizer(config, dataset, language):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(language))\n",
    "    if not tokenizer_path.exists():\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It only has the train split, so we divide it overselves\n",
    "dataset_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "# Build tokenizers\n",
    "tokenizer_src = get_or_build_tokenizer(config, dataset_raw, config['lang_src'])\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, dataset_raw, config['lang_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'love', 'a', 'cat', '.']\n",
      "Token IDs: [9, 194, 11, 1812, 7]\n"
     ]
    }
   ],
   "source": [
    "# Encode a sample sentence\n",
    "encoding = tokenizer_src.encode(\"I love a cat.\")\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", encoding.tokens) # 提供了分词后的词元列表\n",
    "# Print the token IDs\n",
    "print(\"Token IDs:\", encoding.ids) # 提供了每个词元对应的在分词器词典中的索引ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'datasource': 'Helsinki-NLP/opus-100',\n",
    "    'lang_src': 'en',\n",
    "    'lang_tgt': 'zh',\n",
    "    'tokenizer_file': 'tokenizer_{0}.json'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 65.4k/65.4k [00:00<00:00, 107kB/s] \n",
      "Downloading data: 100%|██████████| 355k/355k [00:02<00:00, 149kB/s]\n",
      "Downloading data: 100%|██████████| 143M/143M [00:16<00:00, 8.73MB/s] \n",
      "Downloading data: 100%|██████████| 359k/359k [00:02<00:00, 176kB/s]\n",
      "Generating test split: 100%|██████████| 2000/2000 [00:00<00:00, 40557.20 examples/s]\n",
      "Generating train split: 100%|██████████| 1000000/1000000 [00:00<00:00, 2524245.50 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 454963.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# It only has the train split, so we divide it overselves\n",
    "dataset_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "# Build tokenizers\n",
    "tokenizer_src = get_or_build_tokenizer(config, dataset_raw, config['lang_src'])\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, dataset_raw, config['lang_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['阿拉伯国家联盟首脑级理事会']\n",
      "Token IDs: [8294]\n"
     ]
    }
   ],
   "source": [
    "# Encode a sample sentence\n",
    "encoding = tokenizer_tgt.encode(\"阿拉伯国家联盟首脑级理事会\")\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", encoding.tokens) # 提供了分词后的词元列表\n",
    "# Print the token IDs\n",
    "print(\"Token IDs:\", encoding.ids) # 提供了每个词元对应的在分词器词典中的索引ID。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
