{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大型語言模型（LLM，如BERT、GPT等）處理文本數據的一般流程。這個過程可以細分為以下幾個步驟：\n",
    "\n",
    "1. **分詞（Tokenization）**：\n",
    "   - 這一步驟是將文本字符串分解成較小的單位，稱為“詞元”（tokens）。分詞可以根據不同的規則進行，如基於空格和標點符號分割、子詞分割（如Byte-Pair Encoding, BPE）或使用更複雜的基於規則或學習的方法。\n",
    "\n",
    "2. **詞元到ID的映射（Token to ID mapping）**：\n",
    "   - 分詞後，每個詞元會被轉換成一個唯一的數字ID。這些ID對應於語言模型的詞彙表中的索引。這一步是必要的，因為計算機模型不能直接處理文本數據，而是處理數字。\n",
    "\n",
    "3. **嵌入層（Embedding）**：\n",
    "   - 在將詞元轉換為ID之後，這些ID將被用來在嵌入層中查找每個詞元的向量表示。嵌入層是一個可訓練的參數矩陣，模型通過學習來調整這些向量，以便更好地捕捉詞義和語法關係。\n",
    "   - 這些嵌入通常包括詞嵌入，可能還包括位置嵌入（指示詞元在句子中的位置）和分段嵌入（特別是在處理多個句子時）。\n",
    "\n",
    "4. **模型處理（Model Processing）**：\n",
    "   - 嵌入向量隨後會被送入語言模型的主體，如Transformer結構。這些模型會通過一系列的層處理這些向量，每一層都包括例如自注意力機制和前饋神經網絡等組件。模型通過這種方式來理解和生成文本上下文的表示。\n",
    "\n",
    "5. **輸出解析（Output Decoding）**：\n",
    "   - 對於生成任務，如文本續寫，模型的輸出通常是在詞彙表上的概率分布，這些概率表明下一個詞元是什麼。然後可以使用不同的策略（如貪婪搜索、束搜索）從這些概率中選擇詞元來生成文本。\n",
    "   - 對於分類任務（如情感分析），模型輸出通常是分類標籤的概率。\n",
    "\n",
    "6. **後處理（Post-processing）**：\n",
    "   - 在文本生成任務中，輸出的詞元序列通常會經過後處理轉換回可讀的文本。這可能包括去除特殊詞元、調整格式等步驟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer - Wordlevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'datasource': 'opus_books',\n",
    "    'lang_src': 'en',\n",
    "    'lang_tgt': 'it',\n",
    "    'tokenizer_file': 'tokenizer_{0}.json'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset, language):\n",
    "    for item in dataset:\n",
    "        yield item['translation'][language]\n",
    "\n",
    "def get_or_build_tokenizer(config, dataset, language):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(language))\n",
    "    if not tokenizer_path.exists():\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It only has the train split, so we divide it overselves\n",
    "dataset_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "# Build tokenizers\n",
    "tokenizer_src = get_or_build_tokenizer(config, dataset_raw, config['lang_src'])\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, dataset_raw, config['lang_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'love', 'a', 'cat', '.']\n",
      "Token IDs: [9, 194, 11, 1812, 7]\n"
     ]
    }
   ],
   "source": [
    "# Encode a sample sentence\n",
    "encoding = tokenizer_src.encode(\"I love a cat.\")\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", encoding.tokens) # 提供了分词后的词元列表\n",
    "# Print the token IDs\n",
    "print(\"Token IDs:\", encoding.ids) # 提供了每个词元对应的在分词器词典中的索引ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "- https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'Source: Project Gutenberg',\n",
       "  'it': 'Source: www.liberliber.it/Audiobook available here'},\n",
       " {'en': 'Jane Eyre', 'it': 'Jane Eyre'},\n",
       " {'en': 'Charlotte Bronte', 'it': 'Charlotte Brontë'},\n",
       " {'en': 'CHAPTER I', 'it': 'PARTE PRIMA'},\n",
       " {'en': 'There was no possibility of taking a walk that day.',\n",
       "  'it': 'I. In quel giorno era impossibile passeggiare.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "dataset_raw['translation'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import BilingualDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input': tensor([  2, 298, 845,  ...,   1,   1,   1]),\n",
       " 'decoder_input': tensor([  2, 269, 932,  ...,   1,   1,   1]),\n",
       " 'encoder_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
       " 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
       " 'label': tensor([269, 932,   3,  ...,   1,   1,   1]),\n",
       " 'src_text': 'Jane Eyre',\n",
       " 'tgt_text': 'Jane Eyre'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假設資料集和分詞器已經定義\n",
    "dataset = BilingualDataset(dataset_raw, tokenizer_src, tokenizer_tgt, 'en', 'it', 4096)\n",
    "sample = dataset[1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
